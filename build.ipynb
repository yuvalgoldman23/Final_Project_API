{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a78cef-97ee-49bb-a205-10cb75d10d43",
   "metadata": {},
   "source": [
    "## Theory: Predicting Movie Preferences\n",
    "\n",
    "### The Hypothesis\n",
    "My theory is that a person's taste in movies can be predicted. Specifically, if someone liked one movie, it is possible to predict whether they would like another. For example, if someone enjoyed a good action movie, they are likely to enjoy another action movie.\n",
    "\n",
    "### The Idea\n",
    "The idea is to build a neural network that takes a comparison of two movies as input and predicts whether the movies are compatible or not.\n",
    "\n",
    "### Definition of Compatible Movies\n",
    "In my theory, two movies are considered compatible if:\n",
    "1. If I liked one of them, I would like the other.\n",
    "2. If I hated one of them, I would hate the other.\n",
    "\n",
    "Conversely, movies are considered incompatible if:\n",
    " If I liked one of them, I would hate the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27cdacd3-4f47-4764-bcad-36e353f346c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import hashlib\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf03001-fc71-4cbe-a634-ed1def2857f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"root\",\n",
    "    password=\"yanovsky\",\n",
    "    database=\"final_project_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69f2087-7653-4855-8f4f-bee0ba3007b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL database\n"
     ]
    }
   ],
   "source": [
    "if connection.is_connected():\n",
    "    print(\"Connected to MySQL database\")\n",
    "    cursor = connection.cursor()\n",
    "    cursor2 = connection.cursor(dictionary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332baf24-37ee-4ffe-b065-ba8269cb0685",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "### The Dataset\n",
    "The first step was to collect movies and TV shows to work with. Due to time constraints and the limitations of the tmdb api I used to gather the data, I decided to focus on the most popular movies and TV shows.\n",
    "\n",
    "### Reasoning\n",
    "The rationale behind this choice is that popular movies and series are more likely to come to mind when thinking about specific examples. As a result, there is a high probability that the dataset will include movies or series that most people are familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e3fd46-2835-4ed2-bc1d-4bc84f50cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of media collected: 13938\n"
     ]
    }
   ],
   "source": [
    "query=f\"SELECT COUNT(*) AS row_count from media_data\"\n",
    "cursor2.execute(query)\n",
    "print (\"The number of media collected: \" + str(cursor2.fetchall()[0][\"row_count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665b850-06b4-4a2a-b60c-fb0d223bb2ea",
   "metadata": {},
   "source": [
    "## User Collection\n",
    "\n",
    "### The Process\n",
    "The websiteâ€™s user interface provided the id of the media (movies or TV shows) in IMDb. Using a web crawler, I was able to collect a large number of users who interacted with or provided feedback on these movies or series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa11075d-97c0-4927-af04-53914382a7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Users collected: 108481\n"
     ]
    }
   ],
   "source": [
    "query=f\"SELECT COUNT(*) AS row_count from users\"\n",
    "cursor2.execute(query)\n",
    "print (\"The number of Users collected: \" + str(cursor2.fetchall()[0][\"row_count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da43aa6b-c580-445b-b42d-0937731f305d",
   "metadata": {},
   "source": [
    "## Collecting Rating Lists\n",
    "\n",
    "### Purpose\n",
    "To create a system capable of generating realistic insights, it was necessary to collect a significant number of real user rating lists. This data allows us to learn what each person liked or disliked and draw connections based on that information.\n",
    "\n",
    "### The Process\n",
    "After gathering data from 100,000 users, I focused on collecting ratings only for the movies and TV shows stored in my database. While this approach might seem to disregard a substantial number of ratings, it is worth noting that the stored media were the most popular ones. As a result, approximately 50% of the ratings were retained and utilized, ensuring a robust dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222fb117-575f-4a76-8932-f540e5e26654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rating collected: 546145\n"
     ]
    }
   ],
   "source": [
    "query=f\"SELECT COUNT(*) AS row_count from rating\"\n",
    "cursor2.execute(query)\n",
    "print (\"The number of rating collected: \" + str(cursor2.fetchall()[0][\"row_count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72e5c5c7-7ede-49c5-8f63-be4ed390ba27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of users whe have  5 or more ratings: 37362\n",
      "The total number of rating of the users who have  5 or more ratings: 436644\n"
     ]
    }
   ],
   "source": [
    "query=f\"SELECT COUNT(User_ID) AS user_count, SUM(rating_count) AS total_ratings_sum FROM (SELECT User_ID, COUNT(*) AS rating_count FROM rating GROUP BY User_ID HAVING COUNT(*) >= 5) AS subquery;\"\n",
    "cursor2.execute(query)\n",
    "x=cursor2.fetchall()[0]\n",
    "print (\"The number of users whe have  5 or more ratings: \" + str(x[\"user_count\"]))\n",
    "print (\"The total number of rating of the users who have  5 or more ratings: \" + str(x[\"total_ratings_sum\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6a8c6-3de5-46cc-a82c-0efd43259532",
   "metadata": {},
   "source": [
    "## Movie Representation: Matrix Idea\n",
    "\n",
    "### The Challenge\n",
    "I needed to devise a way to represent each movie as a matrix so that similar movies would have similar matrices. Additionally, the representation needed to provide a wide range of diversity to capture meaningful differences.\n",
    "\n",
    "### The Solution\n",
    "I decided to represent movies using matrices based on their genres. The tmdb api provided the genres associated with each movie, along with a hierarchy indicating which genres are more dominant in the movie.\n",
    "\n",
    "### Representation Details\n",
    "- There are 19 genres in total.\n",
    "- Each movie can be represented by up to 5 genres.\n",
    "- The matrix representation of a movie is based on this genre information, with the hierarchy dictating the order and weight of the genres.\n",
    "\n",
    "This representation ensures that similar movies (in terms of genre) will have similar matrices, providing a meaningful way to compare them.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e50cd662-809a-47c6-956f-93f6ff66244d",
   "metadata": {},
   "source": [
    "{\n",
    "  \"adult\": false,\n",
    "  \"backdrop_path\": \"/s3TBrRGB1iav7gFOCNx3H31MoES.jpg\",\n",
    "  \"belongs_to_collection\": null,\n",
    "  \"budget\": 160000000,\n",
    "  \"genres\": [\n",
    "    {\n",
    "      \"id\": 28,\n",
    "      \"name\": \"Action\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": 878,\n",
    "      \"name\": \"Science Fiction\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": 12,\n",
    "      \"name\": \"Adventure\"\n",
    "    }\n",
    "  ],\n",
    "  \"homepage\": \"http://inceptionmovie.warnerbros.com/\",\n",
    "  \"id\": 27205,\n",
    "  \"imdb_id\": \"tt1375666\",\n",
    "  \"original_language\": \"en\",\n",
    "  \"original_title\": \"Inception\",\n",
    "  \"overview\": \"A thief who steals corporate secrets through the use of dream-sharing technology is given the inverse task of planting an idea into the mind of a CEO.\",\n",
    "  \"popularity\": 82.97,\n",
    "  \"poster_path\": \"/qmDpIHrmpJINaRKAfWQfftjCdyi.jpg\",\n",
    "  \"production_companies\": [\n",
    "    {\n",
    "      \"id\": 923,\n",
    "      \"logo_path\": \"/5UQsZrfbfG2dYJbx8DxfoTr2H2B.png\",\n",
    "      \"name\": \"Legendary Pictures\",\n",
    "      \"origin_country\": \"US\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": 9996,\n",
    "      \"logo_path\": \"/3tvBqYsBhxWeHlu62SIJ1el93O7.png\",\n",
    "      \"name\": \"Syncopy\",\n",
    "      \"origin_country\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ac26cbf-e89d-486e-9a5c-a37815fe4ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct movie represtation: 2792\n"
     ]
    }
   ],
   "source": [
    "query=f\"SELECT COUNT(DISTINCT genres) AS unique_genres_count FROM media_data;\"\n",
    "cursor2.execute(query)\n",
    "x=cursor2.fetchall()[0]\n",
    "print (\"The number of distinct movie represtation: \" +str(x[\"unique_genres_count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8297822-1591-4ea1-9d2f-185c10197c96",
   "metadata": {},
   "source": [
    "## Collecting Users and Their Ratings as Objects for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c69acde6-bba0-4d7a-a5ad-3f5fc3e672e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=f\"SELECT rating.*, media_data.genres FROM rating JOIN media_data ON rating.media_ID = media_data.id;\"\n",
    "cursor2.execute(query)\n",
    "\n",
    "rating_results= cursor2.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d36f0f-a71f-4c31-ac47-038675b689e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72f70611-769c-4f10-b619-c046bac352d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=f\"SELECT * FROM media_data \"\n",
    "cursor2.execute(query)\n",
    "\n",
    "movie_data= cursor2.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc1e484c-8121-4124-91bc-d9797bafbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=f\"SELECT id from users\"\n",
    "cursor2.execute(query)\n",
    "users=cursor2.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "678f33ea-6678-4de5-a306-d7fd81b34da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = defaultdict(list)\n",
    "for rating in rating_results:\n",
    "    grouped_results[rating['User_ID']].append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11c2968c-d0b3-4cb7-b87a-11afae72055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings for User_ID 103193794990463432280 :\n",
      "{'ID': 'aad3e99c6332', 'media_ID': '440021', 'is_movie': 1, 'User_ID': '104471337628996976085', 'rating': 5, 'rating_date': datetime.datetime(2024, 9, 22, 23, 50, 15), 'genres': '27,9648,35,53,'}\n",
      "{'ID': 'e72dd716ad41', 'media_ID': '446354', 'is_movie': 1, 'User_ID': '104471337628996976085', 'rating': 7, 'rating_date': datetime.datetime(2024, 9, 22, 23, 50, 15), 'genres': '18,36,'}\n",
      "{'ID': 'fdd9e348d81b', 'media_ID': '274857', 'is_movie': 1, 'User_ID': '104471337628996976085', 'rating': 8, 'rating_date': datetime.datetime(2024, 9, 22, 23, 50, 15), 'genres': '28,18,14,'}\n",
      "{'ID': 'd110e29f585c', 'media_ID': '4442', 'is_movie': 1, 'User_ID': '104471337628996976085', 'rating': 6, 'rating_date': datetime.datetime(2024, 9, 22, 23, 50, 15), 'genres': '12,14,28,35,53,'}\n",
      "{'ID': '039b23105dbb', 'media_ID': '381719', 'is_movie': 1, 'User_ID': '104471337628996976085', 'rating': 7, 'rating_date': datetime.datetime(2024, 9, 22, 23, 50, 15), 'genres': '16,12,10751,'}\n",
      "{'ID': '4eaae36bfec0', 'media_ID': '33001', 'is_movie': 0, 'User_ID': '104471337628996976085', 'rating': 3, 'rating_date': datetime.datetime(2024, 9, 22, 23, 50, 15), 'genres': '18,14,878,9648,'}\n",
      "{'ID': 'ae16d26834d7', 'media_ID': '10147', 'is_movie': 1, 'User_ID': '104471337628996976085', 'rating': 8, 'rating_date': datetime.datetime(2024, 9, 22, 23, 50, 15), 'genres': '18,35,80,'}\n",
      "{'ID': '825f2f875584', 'media_ID': '58811', 'is_movie': 0, 'User_ID': '104471337628996976085', 'rating': 6, 'rating_date': datetime.datetime(2024, 9, 22, 23, 50, 15), 'genres': '18,14,878,'}\n",
      "{'ID': '8a8a896ec78d', 'media_ID': '48138', 'is_movie': 1, 'User_ID': '104471337628996976085', 'rating': 5, 'rating_date': datetime.datetime(2024, 9, 22, 23, 50, 15), 'genres': '28,9648,53,'}\n"
     ]
    }
   ],
   "source": [
    "user_ratings = grouped_results.get(users[543][\"id\"])\n",
    "if user_ratings:\n",
    "    print(f\"Ratings for User_ID 103193794990463432280 :\")\n",
    "    for rating in user_ratings:\n",
    "        print(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10172ec1-4fa3-416a-84e9-0ea94ff6bb27",
   "metadata": {},
   "source": [
    "## Explanation of Matrix Representation\n",
    "\n",
    "### Structure of the Matrices\n",
    "- There are 19 genres, each represented as a column.\n",
    "- Each movie can have up to 5 hierarchical levels of genres, represented as rows.\n",
    "- The matrix has 19 columns and 5 rows.\n",
    "- Each row contains a `1` in the column corresponding to the genre in the hierarchy for the movie.\n",
    "\n",
    "### Movie Comparison\n",
    "The matrix representation allows for comparing two movies to determine their compatibility. This is the basis for training the neural network to predict whether two movies are compatible.\n",
    "\n",
    "### Compatibility Matrices\n",
    "To assess compatibility, the matrices of two movies are compared using the formula:\n",
    "\n",
    "\n",
    "(x - y) *  (x - y)\n",
    "\n",
    "\n",
    "Where:\n",
    "- \\(x\\) and \\(y\\) are the matrices representing two movies.\n",
    "\n",
    "### Scalability\n",
    "- With 19 genres and up to 5 levels, there are over 2,500 unique matrix representations for individual movies.\n",
    "- When comparing these matrices, the potential combinations lead to millions of unique compatibility matrices, providing a robust dataset for training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed88e00f-2660-4acd-b4e2-14e8e70f0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_genre_matrix(genres_str):\n",
    "        genre_ids = [int(id) for id in genres_str.split(',') if id]\n",
    "        # Number of rows and columns\n",
    "        num_rows = 5\n",
    "        num_cols = 19  # TMDb has 19 standard movie genres\n",
    "        matrix = np.zeros((num_rows, num_cols), dtype=int)\n",
    "        # Complete mapping of TMDb genre IDs to column indices\n",
    "        genre_id_to_col = {\n",
    "        28: 0,   # Action\n",
    "        12: 1,   # Adventure\n",
    "        16: 2,   # Animation\n",
    "        35: 3,   # Comedy\n",
    "        80: 4,   # Crime\n",
    "        99: 5,   # Documentary\n",
    "        18: 6,   # Drama\n",
    "        10751: 7, # Family\n",
    "        14: 8,   # Fantasy\n",
    "        36: 9,   # History\n",
    "        27: 10,  # Horror\n",
    "        10402: 11, # Music\n",
    "        9648: 12, # Mystery\n",
    "        10749: 13, # Romance\n",
    "        878: 14,  # Science Fiction\n",
    "        10770: 15, # TV Movie\n",
    "        53: 16,  # Thriller\n",
    "        10752: 17, # War\n",
    "        37: 18   # Western\n",
    "        }\n",
    "        limited_genre_ids = genre_ids[:num_rows]\n",
    "        # Populate the matrix with one-hot encoding in each row\n",
    "        for i, genre_id in enumerate(limited_genre_ids):\n",
    "         col_index = genre_id_to_col.get(genre_id)\n",
    "         if col_index is not None:\n",
    "            matrix[i, col_index] = 1.0\n",
    "\n",
    "        return matrix.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5e7f00b-29b8-4019-bbf6-afa93e49c36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "[[1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x=create_genre_matrix(\"28,12,878,\")\n",
    "y=create_genre_matrix(\"16,10751,12,14,35,\")\n",
    "print(x)\n",
    "print(\"\\n\")\n",
    "print (y)\n",
    "print(\"\\n\")\n",
    "print((x-y)*(x-y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e7fa95a-0838-40c2-9ab2-d071cf23335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users to train: 26577\n",
      "Number of users to test: 11968\n"
     ]
    }
   ],
   "source": [
    "test_users=[]\n",
    "training_usres=[]\n",
    "trainnum=0\n",
    "testnum=0\n",
    "for usr in users:\n",
    "    try:\n",
    "     if len (grouped_results.get(usr[\"id\"])) >=5:\n",
    "         if random.randint(0, 100) < 70:\n",
    "           training_usres.append(usr[\"id\"])\n",
    "           trainnum=trainnum+ len (grouped_results.get(usr[\"id\"]))\n",
    "         else:\n",
    "               test_users.append(usr[\"id\"])\n",
    "               testnum=testnum+ len (grouped_results.get(usr[\"id\"]))\n",
    "    except Exception  as e:\n",
    "        t=5\n",
    "print(\"Number of users to train: \"+ str(len(training_usres)))\n",
    "print(\"Number of users to test: \"+ str(len(test_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5f843f3-962c-4c0b-9cbb-60db34feace9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100004649219881563245\n",
      "the number of rating for training:319388\n"
     ]
    }
   ],
   "source": [
    "print(training_usres[0])\n",
    "print(\"the number of rating for training:\" +str(trainnum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7330556-f30f-4413-bc97-b26ee3a476c9",
   "metadata": {},
   "source": [
    "## Generating Training Data for the Neural Network\n",
    "\n",
    "### Step 1: Assigning Compatibility Labels\n",
    "The first step in creating the training data is to assign a compatibility label to each pair of movies based on user preferences:\n",
    "\n",
    "- **Label 1 (Compatible)**: If the user liked both movies or disliked both movies.\n",
    "- **Label 0 (Incompatible)**: If the user liked one movie but disliked the other.\n",
    "\n",
    "This labeling process ensures that the neural network learns to predict whether two movies are compatible based on user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32b0d15a-d7ff-467a-9e15-eb0a045229f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training matrix at stage 1 :4715822\n"
     ]
    }
   ],
   "source": [
    "trainingdata=[]\n",
    "trainingdatamatrix=[]\n",
    "totallabels=0\n",
    "numlable1=0\n",
    "for usr in training_usres:\n",
    "    user_ratings = grouped_results.get(usr)\n",
    "    for rating in user_ratings:\n",
    "        for rating2 in user_ratings:\n",
    "            x =  create_genre_matrix(rating['genres'])\n",
    "            y=  create_genre_matrix(rating2['genres'])\n",
    "            lable =0\n",
    "            if rating['rating'] >=7 and  rating2['rating'] >=7:\n",
    "                lable =1\n",
    "            if rating['rating'] <7 and  rating2['rating'] <7:\n",
    "                lable =1 \n",
    "            numlable1=numlable1 +lable\n",
    "            totallabels=totallabels+1\n",
    "            mat=(x-y)*(x-y)\n",
    "            #trainingdata.append((torch.FloatTensor( np.concatenate([x,y])),torch.FloatTensor([lable]))) \n",
    "            trainingdata.append(( mat,lable) )\n",
    "            #if  not any(np.array_equal(matrix, mat) for matrix in trainingdatamatrix):\n",
    "             #trainingdatamatrix.append(mat)\n",
    "print (\"Number of training matrix at stage 1 :\" + str(len(trainingdata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf5e380-fa58-4c50-aeb4-dbb45f38841c",
   "metadata": {},
   "source": [
    "### Step 2: Compressing the Data\n",
    "\n",
    "After assigning compatibility labels, the next step is to compress the information:\n",
    "\n",
    "1. **Matrix Compression**: Compress the compatibility matrices into unique representations to reduce redundancy.\n",
    "2. **Label Count**: For each unique representation:\n",
    "   - Count how many times it received the label `1` (compatible).\n",
    "   - Count how many times it received the label `0` (incompatible).\n",
    "\n",
    "This step helps to efficiently structure the data and balance the training dataset for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b61dd97-d6de-45f3-b73b-5184c84d2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_label_count = defaultdict(lambda: {'label_1': 1, 'label_0': 1})\n",
    "\n",
    "# Process each tuple (matrix, label)\n",
    "for matrix, label in trainingdata:\n",
    "    # Convert matrix to a hashable type (tuple of tuples)\n",
    "    matrix_hashable = tuple(map(tuple, matrix))\n",
    "    \n",
    "    # Update counts based on the label\n",
    "    if label == 1:\n",
    "        matrix_label_count[matrix_hashable]['label_1'] += 1\n",
    "    elif label == 0:\n",
    "        matrix_label_count[matrix_hashable]['label_0'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dca91be-0447-448a-a3a9-dfdfa31d0b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = list(matrix_label_count.items())[0]\n",
    "matrix_data_traing=list(matrix_label_count.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "603db832-b6f8-4d43-91b0-5f90b2410b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)), {'label_1': 461, 'label_0': 229})\n"
     ]
    }
   ],
   "source": [
    "print (matrix_data_traing[345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a640a36-6a6a-424c-8b70-909f79f18a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_1': 398145, 'label_0': 37153}\n"
     ]
    }
   ],
   "source": [
    "print (a)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3153bbd2-86d7-4b01-bb33-d6d829e0a12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322059\n",
      "4783940\n"
     ]
    }
   ],
   "source": [
    "print (len(matrix_data_traing))\n",
    "print (len(trainingdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36a989-2050-4513-b7d6-0b688fa66f4d",
   "metadata": {},
   "source": [
    "### Step 3: Converting Labels to Probabilities\n",
    "\n",
    "The final step is to transform the compatibility labels into probabilities:\n",
    "\n",
    "1. **Probability Calculation**: \n",
    "   - For each unique matrix representation, calculate the estimated probabilities:\n",
    "     - The probability of being compatible (`label 1`) is computed as the ratio of times it received label `1` to the total occurrences.\n",
    "     - The probability of being incompatible (`label 0`) is computed similarly.\n",
    "\n",
    "2. **Training Matrix Filtering**:\n",
    "   - Exclude matrix representations that appeared only a few times, as they do not provide reliable probability estimates.\n",
    "   - Retain only the matrices with sufficient occurrences for robust training data.\n",
    "\n",
    "This process ensures that the neural network is trained to predict the likelihood of compatibility and incompatibility for comparison matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ebca122-32e0-4a27-b81e-523a6903fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_triplets_for_training = []\n",
    "\n",
    "# Iterate through the dictionary to extract matrix and label counts\n",
    "for matrix, counts in matrix_data_traing:\n",
    "    label_1_count = counts['label_1']\n",
    "    label_0_count = counts['label_0']\n",
    "    total_count = label_1_count + label_0_count\n",
    "    \n",
    "    if total_count > 7:\n",
    "        label_1_ratio = label_1_count / total_count\n",
    "        label_0_ratio = label_0_count / total_count\n",
    "        \n",
    "        # Create the triplet and add to the list\n",
    "        matrix_triplet = (matrix, label_1_ratio, label_0_ratio)\n",
    "        matrix_triplets_for_training.append(matrix_triplet)\n",
    "\n",
    "# Print the results\n",
    "#for triplet in matrix_triplets[:5]:  # Display the first 5 triplets\n",
    "  #  print(triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48ce0671-1434-4efc-acf1-6828de681000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0))\n",
      "\n",
      "\n",
      "chance to get 1: 0.5491803278688525\n",
      "\n",
      "\n",
      "chance to get 0: 0.45081967213114754\n"
     ]
    }
   ],
   "source": [
    "a ,b ,c= matrix_triplets_for_training[564]\n",
    "print (a)\n",
    "print(\"\\n\")\n",
    "print (\"chance to get 1: \" +str(b) )\n",
    "print (\"\\n\")\n",
    "print (\"chance to get 0: \" +str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0133d7be-e45f-44e3-80ce-04678aeea883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data at stage 3: 105794\n"
     ]
    }
   ],
   "source": [
    "print (\"Training data at stage 3: \"+ str(len (matrix_triplets_for_training)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e74b7ed9-0155-4f9b-9e45-a19b72aee723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(5 * 19, 128)  # Flatten 5x19 input to 95 features, then 128 neurons in the first layer\n",
    "        self.fc2 = nn.Linear(128, 64)      # Second layer with 64 neurons\n",
    "        self.fc3 = nn.Linear(64, 64)       # Third layer with 64 neurons\n",
    "        self.fc4 = nn.Linear(64, 32)       # Fourth layer with 32 neurons\n",
    "        self.fc5 = nn.Linear(32, 2)        # Output layer for 2 classes (class 1 and class 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 5 * 19)                   # Flatten the matrix input\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)  # Leaky ReLU with negative slope 0.01\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)  # Leaky ReLU for second layer\n",
    "        x = F.leaky_relu(self.fc3(x), negative_slope=0.01)  # Leaky ReLU for third layer\n",
    "        x = F.leaky_relu(self.fc4(x), negative_slope=0.01)  # Leaky ReLU for fourth layer\n",
    "        x = torch.softmax(self.fc5(x), dim=1)    # Softmax output for class probabilities\n",
    "        #x = torch.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = DNNModel()\n",
    "\n",
    "# Use KLDivLoss for probability distributions\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = torch.nn.KLDivLoss(reduction='batchmean')  # KL Divergence Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d83a3e26-12cd-4f55-a953-ddf625a0b703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0))\n",
      "[0.9146492747497117, 0.0853507252502883]\n"
     ]
    }
   ],
   "source": [
    "matrices = [triplet[0] for triplet in matrix_triplets_for_training]  # 5x19 matrices\n",
    "labels = [[triplet[1], triplet[2]] for triplet in matrix_triplets_for_training]  # Probabilities for label 1 and label 0\n",
    "print (matrices[0])\n",
    "print  (labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dddd06e8-c3f0-4949-965e-0e1ea351af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_tensor = torch.tensor(matrices, dtype=torch.float32)  # Ensure float type\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32)      # Float for softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69b8cd0b-048b-4091-a259-0dc072754294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixTripletDataset(Dataset):\n",
    "    def __init__(self, matrices, labels):\n",
    "        self.matrices = matrices\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.matrices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to tensors\n",
    "        matrix = torch.tensor(self.matrices[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)  # Probabilities for both class 1 and class 0\n",
    "        return matrix, label\n",
    "\n",
    "# Create the dataset\n",
    "dataset = MatrixTripletDataset(matrices, labels)\n",
    "\n",
    "# Create DataLoader instance\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9292270c-067f-486f-b956-93fcd4f2c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Trains the neural network model.\n",
    "    \n",
    "    Args:\n",
    "    - model: The neural network model to be trained.\n",
    "    - train_loader: DataLoader containing the training data.\n",
    "    - criterion: Loss function.\n",
    "    - optimizer: Optimizer for updating model weights.\n",
    "    - num_epochs: Number of epochs to train the model (default = 10).\n",
    "    \n",
    "    Returns:\n",
    "    - model: The trained model.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0  # To accumulate loss for each epoch\n",
    "        \n",
    "        # Iterate over the batches in train_loader\n",
    "        for inputs, target_probs in train_loader:\n",
    "            # Ensure inputs and target_probs are in the correct format (float32)\n",
    "            inputs, target_probs = inputs.to(torch.float32), target_probs.to(torch.float32)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: compute model's predictions\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss (difference between predicted and true probabilities)\n",
    "            loss = criterion(outputs, target_probs)\n",
    "            \n",
    "            # Backward pass: compute the gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate the running loss\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss for the epoch and print it\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    print('Training complete.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "200ac161-f929-4a79-a33e-9aea7c755da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.0329\n",
      "Epoch [2/30], Loss: 0.0324\n",
      "Epoch [3/30], Loss: 0.0321\n",
      "Epoch [4/30], Loss: 0.0319\n",
      "Epoch [5/30], Loss: 0.0316\n",
      "Epoch [6/30], Loss: 0.0314\n",
      "Epoch [7/30], Loss: 0.0311\n",
      "Epoch [8/30], Loss: 0.0309\n",
      "Epoch [9/30], Loss: 0.0306\n",
      "Epoch [10/30], Loss: 0.0304\n",
      "Epoch [11/30], Loss: 0.0301\n",
      "Epoch [12/30], Loss: 0.0299\n",
      "Epoch [13/30], Loss: 0.0297\n",
      "Epoch [14/30], Loss: 0.0295\n",
      "Epoch [15/30], Loss: 0.0292\n",
      "Epoch [16/30], Loss: 0.0290\n",
      "Epoch [17/30], Loss: 0.0288\n",
      "Epoch [18/30], Loss: 0.0286\n",
      "Epoch [19/30], Loss: 0.0284\n",
      "Epoch [20/30], Loss: 0.0282\n",
      "Epoch [21/30], Loss: 0.0280\n",
      "Epoch [22/30], Loss: 0.0279\n",
      "Epoch [23/30], Loss: 0.0277\n",
      "Epoch [24/30], Loss: 0.0275\n",
      "Epoch [25/30], Loss: 0.0273\n",
      "Epoch [26/30], Loss: 0.0272\n",
      "Epoch [27/30], Loss: 0.0270\n",
      "Epoch [28/30], Loss: 0.0269\n",
      "Epoch [29/30], Loss: 0.0267\n",
      "Epoch [30/30], Loss: 0.0266\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "trained_model = train_model(model, train_loader, criterion, optimizer, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e0c7efc-d3fa-4d57-b63b-6b2b07bc4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_compatibility(usr_rating,cannidate):\n",
    "    like=0\n",
    "    dislike=0\n",
    "    movie_entry = next((movie for movie in movie_data if movie['id'] == cannidate), None)\n",
    "    matrix_cannidate= create_genre_matrix(movie_entry['genres'])\n",
    "    if movie_entry:\n",
    "        for rating in usr_rating:\n",
    "            if cannidate != rating[\"media_ID\"]:\n",
    "              matrix_rating=create_genre_matrix(rating['genres'])\n",
    "              mat= (matrix_cannidate-matrix_rating)*(matrix_cannidate-matrix_rating)\n",
    "              mat_tensor= torch.tensor(mat, dtype=torch.float32)\n",
    "              propabillity=trained_model(mat_tensor)\n",
    "              p1=propabillity[0,0].item()\n",
    "              p2=propabillity[0,1].item()\n",
    "              if rating['rating'] >=7:\n",
    "                  like=like+ p1\n",
    "                  dislike=dislike+ p2\n",
    "              else :\n",
    "                    like=like+ p2\n",
    "                    dislike=dislike+ p1\n",
    "    if like >=dislike:\n",
    "        return 1\n",
    "    else:\n",
    "      return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e435f76e-8ae0-4300-812b-b1c3f6c5b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_user_rating_list(id):\n",
    "    TP=0.0\n",
    "    TN=0.0\n",
    "    FP=0.0\n",
    "    FN=0.0\n",
    "    \n",
    "    user_ratings = grouped_results.get(id)\n",
    "    for rating in user_ratings:\n",
    "        b=check_compatibility(user_ratings,rating[\"media_ID\"])\n",
    "        if rating['rating'] >=7:\n",
    "            if b==1:\n",
    "                TP=TP+1\n",
    "            else:\n",
    "                FP=FP+1\n",
    "        else:\n",
    "           if b==0:\n",
    "               TN=TN+1\n",
    "           else:\n",
    "               FN=FN+1\n",
    "    #print (right)\n",
    "    #print(wrong)\n",
    "    return  TP,TN,FP,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0e803d6-db21-4bc2-bc10-830a5bb3e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP=0.0\n",
    "TN=0.0\n",
    "FP=0.0\n",
    "FN=0.0\n",
    "\n",
    "for i in range(int(len(test_users))):\n",
    "   a,b,c,d= check_user_rating_list(test_users[i])\n",
    "   TP=TP +a\n",
    "   TN=TN+b\n",
    "   FP=FP+c\n",
    "   FN=FN+d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff6c6f4f-74ce-45b7-9c3b-0843c65e172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 63115.0\n",
      "TN: 32479.0\n",
      "FP: 20104.0\n",
      "FN: 26625.0\n",
      "Right gues rate: 0.6716693717810895\n",
      "Right positive rate: 0.7584205529987142\n"
     ]
    }
   ],
   "source": [
    "print (\"TP: \" + str (TP ))\n",
    "print (\"TN: \" + str (TN ))\n",
    "print (\"FP: \" + str (FP ))\n",
    "print (\"FN: \" + str (FN ))\n",
    "print (\"Right gues rate: \" + str(((TP+TN)/(TP+TN+FP+FN))))\n",
    "print (\"Right positive rate: \" +str((TP/(TP+FP))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615bbcbf-2052-4ff5-a89c-95dfb548b0fe",
   "metadata": {},
   "source": [
    "## Algorithms for Selecting Good Candidates\n",
    "\n",
    "Even though we now have a method to determine whether a movie is recommended for a user, it is still crucial to start with good candidate movies. For example, if someone loves action movies, recommending a 1960s action movie might still result in a recommendation, but a more relevant candidate would be better.\n",
    "\n",
    "### Two Algorithms for Selecting Candidates\n",
    "\n",
    "#### 1. Popularity-Based Algorithm\n",
    "This algorithm selects candidates based on their popularity:\n",
    "- Filter movies with an average rating above 7.\n",
    "- Count the number of users who rated each movie.\n",
    "- Movies with a higher number of ratings are more likely to be selected as candidates.\n",
    "\n",
    "#### 2. Keyword-Based Algorithm\n",
    "This algorithm uses keywords to find relevant candidates:\n",
    "- The user interface provides keywords associated with each movie.\n",
    "- Collect all the keywords from movies the user liked.\n",
    "- Compare the keywords using natural language processing (NLP):\n",
    "  - Identify frequently occurring keywords.\n",
    "  - If no keyword appears frequently, group similar keywords using NLP techniques and count them as one until a sufficient number of grouped keywords is found.\n",
    "\n",
    "Using the selected keywords:\n",
    "- Find movies that share the same keywords.\n",
    "- Select these movies as candidates for recommendations.\n",
    "\n",
    "These two algorithms ensure a robust approach for selecting relevant movie candidates tailored to user preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6a77a0-0607-4fcd-8423-cc7668222740",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression expected after dictionary key and ':' (3427941.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 27\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"movies\": [\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expression expected after dictionary key and ':'\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "  \"id\": 27205,\n",
    "  \"keywords\": [\n",
    "    {\n",
    "      \"id\": 1234,\n",
    "      \"name\": \"dream\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": 5678,\n",
    "      \"name\": \"subconscious\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": 91011,\n",
    "      \"name\": \"heist\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": 121314,\n",
    "      \"name\": \"time dilation\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "{\n",
    "  \"id\": 1701,\n",
    "  \"name\": \"hero\",\n",
    "  \"movies\": [\n",
    "    {\n",
    "      \"id\": 299536,\n",
    "      \"title\": \"Avengers: Infinity War\",\n",
    "      \"release_date\": \"2018-04-27\",\n",
    "      \"popularity\": 300.0\n",
    "    },\n",
    "    {\n",
    "      \"id\": 299537,\n",
    "      \"title\": \"Captain Marvel\",\n",
    "      \"release_date\": \"2019-03-08\",\n",
    "      \"popularity\": 250.0\n",
    "    }\n",
    "    // Additional movies...\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e91583-dcd3-4c7c-a992-f80c1bb77ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keyword_similarity(keywords):\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity between each pair of keywords in the input array.\n",
    "\n",
    "    Parameters:\n",
    "        keywords (list): A list of dictionaries, each containing 'count', 'id', and 'name' of a keyword.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing pairs of keywords and their similarity score.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for (kw1, kw2) in combinations(keywords, 2):\n",
    "\n",
    "        doc1 = nlp(kw1['name'])\n",
    "        doc2 = nlp(kw2['name'])\n",
    "        similarity = doc1.similarity(doc2)\n",
    "\n",
    "        \n",
    "        if similarity >=0.80:\n",
    "         similarities.append({\n",
    "            \"keyword1\": kw1['name'],\n",
    "            \"keyword2\": kw2['name'],\n",
    "            \"id1\": kw1[\"id\"],\n",
    "            \"id2\":kw2[\"id\"],\n",
    "            \"count1\":kw1[\"count\"],\n",
    "            \"count2\": kw2[\"count\"],\n",
    "            \"similarity\": similarity\n",
    "         })\n",
    "\n",
    "    # Sort by similarity in descending order\n",
    "    similarities = sorted(similarities, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "    return similarities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
